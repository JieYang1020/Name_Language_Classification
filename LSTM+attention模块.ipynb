{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "名字中最长长度为19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BucketIterator object at 0x0000020710270A58>\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from torchtext import data\n",
    "from tqdm import tqdm\n",
    "from torch.nn import init\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "#所有英文字母加上五个标点符号(包含一个空格)\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "# 将unicode转为ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "#build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "# read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "file_path = r'C:\\Users\\lenovo\\Desktop\\Josie\\自学\\Pytorch_名字分类\\data\\data\\names'\n",
    "#数据整合成dataframe\n",
    "total_data = pd.DataFrame(columns = ('content', 'category'))\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for idx, file in enumerate(files):\n",
    "        category = file.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(os.path.join(root, file))\n",
    "        for line in lines:\n",
    "            single_name = {'content':line, 'category':int(idx)}\n",
    "            total_data = total_data.append(single_name, ignore_index = True)\n",
    "#找到最长名字，做pad用(fix_length = 最长长度)\n",
    "max_length = 0\n",
    "for i in total_data['content']:\n",
    "    if len(i) > max_length:\n",
    "        max_length = len(i)\n",
    "print('名字中最长长度为%d'%max_length)\n",
    "#解决loss值不变的方法一：打乱数据集(total_data)\n",
    "total_data = shuffle(total_data)\n",
    "tokenize = lambda x: x.split()\n",
    "#data.Field:定义样本的处理操作\n",
    "TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, fix_length = 19)\n",
    "LABEL = data.Field(sequential = False, use_vocab = False)\n",
    "#将原始的corpus转换成data.Example实例(主要为data.Example.fromlist方法)\n",
    "#都是train数据，就不用区分是train还是test了\n",
    "def get_dataset(content_info, category_info, text_field, label_field):\n",
    "    fields = [('content', text_field),\n",
    "               ('category', label_field)]\n",
    "    examples = []\n",
    "    for text, label in zip(content_info, category_info):\n",
    "        examples.append(data.Example.fromlist([text, label], fields))\n",
    "    return examples, fields\n",
    "train_examples, train_fields = get_dataset(total_data['content'], total_data['category'],\n",
    "                                          TEXT, LABEL)\n",
    "#使用torchtext.data.Dataset来构建数据集\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "vectors = Vectors(name = r'C:\\Users\\lenovo\\.vector_cache\\glove.6B\\glove.6B.300d.txt')\n",
    "TEXT.build_vocab(train, vectors = vectors)\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "#解决loss值不变的方法二：减小batch_size\n",
    "train_iter = BucketIterator(train, batch_size = 64, device = -1,\n",
    "                            sort_key = lambda x: len(x.content),\n",
    "                            sort = False,sort_within_batch = False, repeat = False)\n",
    "print(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的loss值为1.923520\n",
      "第1个epoch的准确率为0.454639\n",
      "第2个epoch的loss值为1.880422\n",
      "第2个epoch的准确率为0.466982\n",
      "第3个epoch的loss值为1.873751\n",
      "第3个epoch的准确率为0.468973\n",
      "第4个epoch的loss值为1.820609\n",
      "第4个epoch的准确率为0.489145\n",
      "第5个epoch的loss值为1.742221\n",
      "第5个epoch的准确率为0.516748\n",
      "第6个epoch的loss值为1.703426\n",
      "第6个epoch的准确率为0.530731\n",
      "第7个epoch的loss值为1.673098\n",
      "第7个epoch的准确率为0.541728\n",
      "第8个epoch的loss值为1.655453\n",
      "第8个epoch的准确率为0.546188\n",
      "第9个epoch的loss值为1.638959\n",
      "第9个epoch的准确率为0.549034\n",
      "第10个epoch的loss值为1.633103\n",
      "第10个epoch的准确率为0.550657\n",
      "第11个epoch的loss值为1.625585\n",
      "第11个epoch的准确率为0.552249\n",
      "第12个epoch的loss值为1.622786\n",
      "第12个epoch的准确率为0.552029\n",
      "第13个epoch的loss值为1.610730\n",
      "第13个epoch的准确率为0.555138\n",
      "第14个epoch的loss值为1.602811\n",
      "第14个epoch的准确率为0.556140\n",
      "第15个epoch的loss值为1.599468\n",
      "第15个epoch的准确率为0.556834\n",
      "第16个epoch的loss值为1.594205\n",
      "第16个epoch的准确率为0.557353\n",
      "第17个epoch的loss值为1.594660\n",
      "第17个epoch的准确率为0.557747\n",
      "第18个epoch的loss值为1.589583\n",
      "第18个epoch的准确率为0.557503\n",
      "第19个epoch的loss值为1.589371\n",
      "第19个epoch的准确率为0.557803\n",
      "第20个epoch的loss值为1.578785\n",
      "第20个epoch的准确率为0.558505\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ef3abc10da69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-ef3abc10da69>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m#zero_grad()将每个epoch的第一个梯度置零\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-ef3abc10da69>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_sentences)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mh_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mc_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_cell_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#在LSTM模块的基础上加了self.attention_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 522\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.batch_size = 64\n",
    "        self.output_size = 18\n",
    "        self.hidden_size = 128\n",
    "        self.vocab_size = len(TEXT.vocab)\n",
    "        self.embedding_length = 300\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weight_matrix, requires_grad = True)\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "#       final_state.size()#([1, 64, 128])、(lstm_output.size())   #([19, 64, 128])\n",
    "        hidden = final_state.squeeze(0)\n",
    "#       hidden.size()   #([64, 128])\n",
    "#         print(hidden.size())\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        lstm_output = lstm_output.permute(1,2,0)\n",
    "        new_hidden_state = torch.bmm(lstm_output,\n",
    "                                    soft_attn_weights.transpose(0, 1).unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state\n",
    "    def forward(self, input_sentences):\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if input.shape[1] == self.batch_size:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, input.shape[1], self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(1, input.shape[1], self.hidden_size))   \n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        #在LSTM模块的基础上加了self.attention_net\n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "        logits = self.fc(attn_output)\n",
    "        return logits\n",
    "\n",
    "#梯度裁剪来防止梯度爆炸\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        #nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def main():\n",
    "    model = AttentionModel()\n",
    "    model.train()\n",
    "    total_epoches_loss = 0\n",
    "    total_epoches_acc = 0\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    #动态监控lr,如多次(200)没有发生loss下降，则降低学习率\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 1e-3)\n",
    "    #传入优化器让学习器受其管理，当连续200次没有减少Loss时就减低lr(乘以0.9)\n",
    "    scheduler = StepLR(optimizer, step_size = 500, gamma = 0.9)\n",
    "    #pytorch中处理多分类用CrossEntropyLoss时，标签需从0开始\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    epoches = 100\n",
    "    Loss_list = []\n",
    "    Accuracy_list = []\n",
    "    for i in range(epoches):\n",
    "        each_batch_loss = 0\n",
    "        each_batch_acc = 0\n",
    "        for epoch, batch in enumerate(train_iter):\n",
    "            #zero_grad()将每个epoch的第一个梯度置零\n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(batch.content)\n",
    "            loss = loss_function(predicted, batch.category)\n",
    "            num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
    "                           == batch.category.data).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            clip_gradient(model, 1e-1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            each_batch_loss += loss.item()\n",
    "            each_batch_acc += acc.item()\n",
    "            Loss_list.append(each_batch_loss)\n",
    "            Accuracy_list.append(each_batch_acc)\n",
    "        total_epoches_loss += each_batch_loss\n",
    "        total_epoches_acc += each_batch_acc\n",
    "        print('第%d个epoch的loss值为%f'%(i+1, (each_batch_loss/len(train_iter))))\n",
    "        print('第%d个epoch的准确率为%f'%(i+1, (each_batch_acc/len(train_iter)/100.0)))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
