{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "名字中最长长度为19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BucketIterator object at 0x00000202D06715C0>\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from torchtext import data\n",
    "from tqdm import tqdm\n",
    "from torch.nn import init\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "#所有英文字母加上五个标点符号(包含一个空格)\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "# 将unicode转为ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "#build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "# read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "file_path = r'C:\\Users\\lenovo\\Desktop\\Josie\\自学\\Pytorch_名字分类\\data\\data\\names'\n",
    "#数据整合成dataframe\n",
    "total_data = pd.DataFrame(columns = ('content', 'category'))\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for idx, file in enumerate(files):\n",
    "        category = file.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(os.path.join(root, file))\n",
    "        for line in lines:\n",
    "            single_name = {'content':line, 'category':int(idx)}\n",
    "            total_data = total_data.append(single_name, ignore_index = True)\n",
    "#找到最长名字，做pad用(fix_length = 最长长度)\n",
    "max_length = 0\n",
    "for i in total_data['content']:\n",
    "    if len(i) > max_length:\n",
    "        max_length = len(i)\n",
    "print('名字中最长长度为%d'%max_length)\n",
    "#解决loss值不变的方法一：打乱数据集(total_data)\n",
    "total_data = shuffle(total_data)\n",
    "tokenize = lambda x: x.split()\n",
    "#data.Field:定义样本的处理操作\n",
    "TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, fix_length = 19)\n",
    "LABEL = data.Field(sequential = False, use_vocab = False)\n",
    "#将原始的corpus转换成data.Example实例(主要为data.Example.fromlist方法)\n",
    "#都是train数据，就不用区分是train还是test了\n",
    "def get_dataset(content_info, category_info, text_field, label_field):\n",
    "    fields = [('content', text_field),\n",
    "               ('category', label_field)]\n",
    "    examples = []\n",
    "    for text, label in zip(content_info, category_info):\n",
    "        examples.append(data.Example.fromlist([text, label], fields))\n",
    "    return examples, fields\n",
    "train_examples, train_fields = get_dataset(total_data['content'], total_data['category'],\n",
    "                                          TEXT, LABEL)\n",
    "#使用torchtext.data.Dataset来构建数据集\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "vectors = Vectors(name = r'C:\\Users\\lenovo\\.vector_cache\\glove.6B\\glove.6B.300d.txt')\n",
    "TEXT.build_vocab(train, vectors = vectors)\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "#解决loss值不变的方法二：减小batch_size\n",
    "train_iter = BucketIterator(train, batch_size = 64, device = -1,\n",
    "                            sort_key = lambda x: len(x.content),\n",
    "                            sort = False,sort_within_batch = False, repeat = False)\n",
    "print(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型一 textCNN\n",
    "#CNN优化方法： 1）运用unsequeeze将数据从3维变为4维，进入conv2d（而不是conv1d),之后\n",
    "                 #再用sequeeze变回想要的size，比直接用conv1d效果更好\n",
    "               #2) 所有卷积层的in_channels大小设置一样，out_channels(及kernel个数)也\n",
    "               #设置一样\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(textCNN, self).__init__()\n",
    "        self.embed_num = 20074\n",
    "        self.embed_dim = 300\n",
    "        self.class_num = 18\n",
    "        #由于从conv1d变为了conv2d，输入通道数是第二维度（1）\n",
    "        self.in_channels = [1, 1, 1]\n",
    "        #这里的out_channels就是kernel_num,即卷积核的个数\n",
    "        #输入输出通道数设置相同大小效果更好\n",
    "        self.out_channels = [16, 16, 16]\n",
    "        self.kernel_sizes = [3, 3, 3]\n",
    "        self.stride = 1\n",
    "        self.padding = 0\n",
    "        self.dropout = 0.5\n",
    "        self.word_embeddings = nn.Embedding(len(TEXT.vocab), self.embed_dim)\n",
    "        #         #指定预训练的词向量(即embedding层的权重)（Glove)\n",
    "        weight_matrix = TEXT.vocab.vectors\n",
    "        self.word_embeddings.weight.data.copy_(weight_matrix)\n",
    "        #反向时不计算embeddin层的梯度(不更新embedding层的权重)，提升模型训练时间，对应优化器有需要调整的地方\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        #进入CNN层\n",
    "#       [64, 1, 19, 300]\n",
    "        self.conv1 = nn.Conv2d(self.in_channels[0], self.out_channels[0],\n",
    "                               (self.kernel_sizes[0], self.embed_dim), self.stride,\n",
    "                               self.padding)\n",
    "        self.conv2 = nn.Conv2d(self.in_channels[1], self.out_channels[1],\n",
    "                              (self.kernel_sizes[1], self.embed_dim), self.stride,\n",
    "                              self.padding)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(self.in_channels[2], self.out_channels[2],\n",
    "                              (self.kernel_sizes[2], self.embed_dim), self.stride,\n",
    "                              self.padding)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        #全连接Linear层做线性变换\n",
    "        self.label = nn.Linear(self.out_channels[2] * 3, self.class_num)\n",
    "    #CNN模块写到一个函数里面\n",
    "    def conv_block(self, input, conv_layer):\n",
    "        conv_out = conv_layer(input)# conv_out.size() = (batch_size, out_channels, embed_dim(updated), 1)\n",
    "        #变为(batch_size, out_channels,embed_dim(updated))\n",
    "        #squeeze只能删除维数为1的维度\n",
    "        activation = F.relu(conv_out.squeeze(3))\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)\n",
    "        return max_out\n",
    "#         max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
    "#         return max_out\n",
    "    def forward(self, input_sentences):\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.transpose(0, 1).contiguous()\n",
    "        input = input.unsqueeze(1)\n",
    "        #input.size() = (batch_size, 1, num_seq, embedding_dim)\n",
    "\n",
    "        #公式(num_seq - kernel.size + 2padding)/stride + 1\n",
    "        #卷积层，里面包含卷积，激活，最大池化\n",
    "        max_out1 = self.conv_block(input, self.conv1)\n",
    "        max_out2 = self.conv_block(input, self.conv2)\n",
    "        max_out3 = self.conv_block(input, self.conv3)\n",
    "        #拼接(整合维度)\n",
    "        all_out = torch.cat((max_out1, max_out2,max_out3),1)\n",
    "#         #dropout层\n",
    "        fc_in = self.dropout(all_out)\n",
    "        logits = self.label(fc_in)\n",
    "#         logits.size() = [batch_size, class_num]\n",
    "        return logits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n",
      "torch.Size([64, 18]) torch.Size([64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ed6e01ff7120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-ed6e01ff7120>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f9e52fde6d19>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_sentences)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m#         return max_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m         return F.embedding(\n\u001b[0;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1506\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "#梯度裁剪来防止梯度爆炸\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        #nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def main():\n",
    "    model = textCNN()\n",
    "    model.train()\n",
    "    total_epoches_loss = 0\n",
    "    total_epoches_acc = 0\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 1e-3)\n",
    "    #传入优化器让学习器受其管理，当连续500次没有减少Loss时就减低lr(乘以0.9)\n",
    "    scheduler = StepLR(optimizer, step_size = 500, gamma = 0.9)\n",
    "    #pytorch中处理多分类用CrossEntropyLoss时，标签需从0开始\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    epoches = 100\n",
    "    Loss_list = []\n",
    "    Accuracy_list = []\n",
    "    for i in range(epoches):\n",
    "        each_batch_loss = 0\n",
    "        each_batch_acc = 0\n",
    "        for epoch, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(batch.content)\n",
    "            loss = loss_function(predicted, batch.category)\n",
    "            print(predicted.size(), batch.category.size())\n",
    "            num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
    "                           == batch.category.data).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            clip_gradient(model, 1e-1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            each_batch_loss += loss.item()\n",
    "            each_batch_acc += acc.item()\n",
    "            Loss_list.append(each_batch_loss)\n",
    "            Accuracy_list.append(each_batch_acc)\n",
    "        total_epoches_loss += each_batch_loss\n",
    "        total_epoches_acc += each_batch_acc\n",
    "        print('第%d个epoch的loss值为%f'%(i+1, (each_batch_loss/len(train_iter))))\n",
    "        print('第%d个epoch的准确率为%f'%(i+1, (each_batch_acc/len(train_iter)/100.0)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型二\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.batch_size = 64\n",
    "        self.output_size = 18\n",
    "        self.in_channels = 1\n",
    "        self.out_channels = 1\n",
    "        self.kernel_heights = [3, 3, 3]\n",
    "        self.stride = 0\n",
    "        self.padding = 2\n",
    "        self.vocab_size = 19\n",
    "        self.embedding_length = 300\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_length)\n",
    "#         self.word_embeddings.weight = nn.Parameter(weights, requires_grad = False)\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.out_channels,\n",
    "                                                           (self.kernel_heights[0], \n",
    "                                                           self.embedding_length),\n",
    "                                                           self.stride, self.padding)\n",
    "        self.conv2 = nn.Conv2d(self.in_channels, self.out_channels, \n",
    "                                                          (self.kernel_heights[1],\n",
    "                                                          self.embedding_length),\n",
    "                                                          self.stride, self.padding)\n",
    "        self.conv3 = nn.Conv2d(self.in_channels, self.out_channels, \n",
    "                                                         (self.kernel_heights[2],\n",
    "                                                          self.embedding_length),\n",
    "                                                          self.stride, self.padding)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.label = nn.Linear(len(self.kernel_heights)*self.out_channels, \n",
    "                               self.output_size)\n",
    "    def conv_block(self, input, conv_layer):\n",
    "        conv_out = conv_layer(input)\n",
    "        activation = F.relu(conv_out.squeeze(3))\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)\n",
    "        return max_out\n",
    "    def forward(self, input_sentences, batch_size = None):\n",
    "        print(input_sentences.size())\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        \n",
    "        input = input.unsqueeze(1)\n",
    "        max_out1 = self.conv_block(input, self.conv1)\n",
    "        max_out2 = self.conv_block(input, self.conv2)\n",
    "        max_out3 = self.conv_block(input, self.conv3)\n",
    "        \n",
    "        all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
    "        fc_in = self.dropout(all_out)\n",
    "        logits = self.label(fc_in)\n",
    "        return logits\n",
    "def main():\n",
    "    model = CNN()\n",
    "    model.train()\n",
    "    total_epoches_loss = 0\n",
    "    total_epoches_acc = 0\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay=1e-5)\n",
    "    #pytorch中处理多分类用CrossEntropyLoss时，标签需从0开始\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    epoches = 5\n",
    "    Loss_list = []\n",
    "    Accuracy_list = []\n",
    "    for i in range(epoches):\n",
    "        each_batch_loss = 0\n",
    "        each_batch_acc = 0\n",
    "        for epoch, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(batch.content)\n",
    "            loss = loss_function(predicted, batch.category)\n",
    "            num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
    "                           == batch.category.data).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            loss.backward()\n",
    "            clip_gradient(model, 1e-1)\n",
    "            optimizer.step()\n",
    "            each_batch_loss += loss.item()\n",
    "            each_batch_acc += acc.item()\n",
    "            Loss_list.append(each_batch_loss)\n",
    "            Accuracy_list.append(each_batch_acc)\n",
    "        total_epoches_loss += each_batch_loss\n",
    "        total_epoches_acc += each_batch_acc\n",
    "        print('第%d个epoch的loss值为%f'%(i+1, (each_batch_loss/len(train_iter))))\n",
    "        print('第%d个epoch的准确率为%f'%(i+1, (each_batch_acc/len(train_iter)/100.0)))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
