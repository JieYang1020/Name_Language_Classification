{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "#所有英文字母加上五个标点符号(包含一个空格)\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "# 将unicode转为ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "#build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "# read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20074, 2)\n",
      "    content category\n",
      "0    Khoury        0\n",
      "1     Nahas        0\n",
      "2     Daher        0\n",
      "3    Gerges        0\n",
      "4    Nazari        0\n",
      "5   Maalouf        0\n",
      "6    Gerges        0\n",
      "7    Naifeh        0\n",
      "8  Guirguis        0\n",
      "9      Baba        0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "file_path = r'C:\\Users\\lenovo\\Desktop\\Josie\\自学\\Pytorch_名字分类\\data\\data\\names'\n",
    "#数据整合成dataframe\n",
    "total_data = pd.DataFrame(columns = ('content', 'category'))\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for idx, file in enumerate(files):\n",
    "        category = file.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(os.path.join(root, file))\n",
    "        for line in lines:\n",
    "            single_name = {'content':line, 'category':int(idx)}\n",
    "            total_data = total_data.append(single_name, ignore_index = True)\n",
    "print(total_data.shape)\n",
    "print(total_data.head(10))\n",
    "#total_data.shape():[20074, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "#找到最长名字，做pad用(fix_length = 最长长度)\n",
    "max_length = 0\n",
    "for i in total_data['content']:\n",
    "    if len(i) > max_length:\n",
    "        max_length = len(i)\n",
    "print(max_length)\n",
    "print(total_data['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解决loss值不变的方法一：打乱数据集(total_data)\n",
    "from sklearn.utils import shuffle\n",
    "total_data = shuffle(total_data)\n",
    "total_data = total_data.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据'文字补齐和预处理学习(代码已调对)'来调整代码\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "from tqdm import tqdm\n",
    "from torch.nn import init\n",
    "tokenize = lambda x: x.split()\n",
    "#data.Field:定义样本的处理操作\n",
    "TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, fix_length = 19)\n",
    "LABEL = data.Field(sequential = False, use_vocab = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将原始的corpus转换成data.Example实例(主要为data.Example.fromlist方法)\n",
    "#都是train数据，就不用区分是train还是test了\n",
    "def get_dataset(content_info, category_info, text_field, label_field):\n",
    "    fields = [('content', text_field),\n",
    "               ('category', label_field)]\n",
    "    examples = []\n",
    "    for text, label in zip(content_info, category_info):\n",
    "        examples.append(data.Example.fromlist([text, label], fields))\n",
    "    return examples, fields\n",
    "train_examples, train_fields = get_dataset(total_data['content'], total_data['category'],\n",
    "                                          TEXT, LABEL)\n",
    "#使用torchtext.data.Dataset来构建数据集\n",
    "train = data.Dataset(train_examples, train_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe, Vectors\n",
    "from torchtext import data\n",
    "vectors = Vectors(name = r'C:\\Users\\lenovo\\.vector_cache\\glove.6B\\glove.6B.300d.txt')\n",
    "TEXT.build_vocab(train, vectors = vectors)\n",
    "weight_matrix = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "#解决loss值不变的方法二：减小batch_size\n",
    "train_iter = BucketIterator(train, batch_size = 64, device = -1,\n",
    "                            sort_key = lambda x: len(x.content),\n",
    "                            sort = False,sort_within_batch = False, repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-96880932c8dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'准确率为%f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_epoch_acc\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-96880932c8dc>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m                        == batch.category.data).float().sum()\n\u001b[0;32m     50\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_corrects\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mclip_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#模型一\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time \n",
    "class SimpleLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, emb_dim = 300, num_linear = 1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers = 1)\n",
    "        self.linear_layers = []\n",
    "        # 中间fc层\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        # 输出层\n",
    "        self.predictor = nn.Linear(hidden_dim, 18)\n",
    "        # 使用归一化加快运算速度\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]  # 选择最后一个output\n",
    "        for layer in self.linear_layers:\n",
    "            feature = self.bn(feature)\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "def main():\n",
    "    nh = 500\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model = SimpleLSTMBaseline(nh, emb_dim = 300)\n",
    "    model.train()\n",
    "#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.001)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay=1e-5)\n",
    "    #pytorch中处理多分类用CrossEntropyLoss时，标签需从0开始\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    for epoch, batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        start = time.time()\n",
    "        predicted = model(batch.content)\n",
    "        loss = loss_function(predicted, batch.category)\n",
    "        num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
    "                       == batch.category.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "    print('loss值为%f'%(total_epoch_loss/len(train_iter)))\n",
    "    print('准确率为%f'%(total_epoch_acc/len(train_iter)))\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss值为1.899778\n",
      "准确率为45.781806\n"
     ]
    }
   ],
   "source": [
    "#模型二\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(len(TEXT.vocab), 300)\n",
    "        self.lstm = nn.LSTM(input_size = 300, hidden_size = 128, num_layers = 1, batch_first = True)\n",
    "        #input layer:[fix_length, batch_size, embedding_size]\n",
    "        #hidden layer:[fix_length, batch_size, hidden_size]\n",
    "        #nn.Linear(hidden_size, output_category)\n",
    "        self.decoder = nn.Linear(128, 18)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out = self.lstm(embeds)[0]\n",
    "#         print(lstm_out.shape)\n",
    "        final = lstm_out[-1]\n",
    "#         final = F.relu(self.decoder(final))\n",
    "        y = self.decoder(final)\n",
    "        return y\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def main():\n",
    "    model = LSTM()\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.001)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay=1e-5)\n",
    "    #pytorch中处理多分类用CrossEntropyLoss时，标签需从0开始\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    for epoch, batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        start = time.time()\n",
    "        predicted = model(batch.content)\n",
    "        loss = loss_function(predicted, batch.category)\n",
    "        num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
    "                       == batch.category.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "    print('平均loss值为%f'%(total_epoch_loss/len(train_iter)))\n",
    "    print('平均准确率为%f'%(total_epoch_acc/len(train_iter)))\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17357\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "----\n",
      "torch.Size([19, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "----\n",
      "torch.Size([19, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "----\n",
      "torch.Size([19, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "----\n",
      "torch.Size([19, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "----\n",
      "torch.Size([19, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "torch.Size([1, 64, 128])\n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6704bc21d2f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-6704bc21d2f8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m                            == batch.category.data).float().sum()\n\u001b[0;32m     94\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_corrects\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m             \u001b[1;31m#nn.utils.clip_grad_norm(model.parameters(), 10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mclip_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#模型三：格式标准，易读，运算速度快(准确率稳定在85.4%左右)\n",
    "#调参过程中重点调的内容有hidden_size, learning_rate, batch_size\n",
    "#添加：1）embedding_dropout，fc_dropout，relu层  没用\n",
    "#添加；2)softmax层: crossentropy方法里封装了softmax，无需另外加softmax层，这样会压缩数据\n",
    "#       造成误差\n",
    "#添加: 3)weight_decay:权重衰减(L2正则化)来防止过拟合 \n",
    "#epoch尽量大(几百个),可以充分训练\n",
    "#模型变好的方法: 1)随机初始化词向量变为加载预训练好的词向量(在模型里面)(即embedding层的权重)，\n",
    "                #反向时embedding层的权重不更新，加快运算速度\n",
    "#还可以优化的地方:现在是取最后一个字母进入linear层，后可以加入pooling层，计算所有字母的\n",
    "#embeding均值或者最大值，进入linear层，充分利用每个词\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.batch_size = 64\n",
    "        self.hidden_size = 128\n",
    "        self.vocab_size = len(TEXT.vocab)\n",
    "        self.embedding_length = 300\n",
    "        self.embedding_dropout = 0.2\n",
    "        self.fc_dropout = 0.1\n",
    "        #bidirectional没用上，会报错\n",
    "        self.output_size = 18\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_length)\n",
    "#         #指定预训练的词向量(即embedding层的权重)（Glove)\n",
    "        weight_matrix = TEXT.vocab.vectors\n",
    "        self.word_embeddings.weight.data.copy_(weight_matrix)\n",
    "        #反向时不计算embeddin层的梯度(不更新embedding层的权重)，提升模型训练时间，对应优化器有需要调整的地方\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "#         self.word_embeddings.weight = nn.Parameter(word_embeddings, require_grad = True)\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size)\n",
    "#         self.embed_dropout = nn.Dropout(self.embedding_dropout)\n",
    "#         self.fc_dropout = nn.Dropout(self.fc_dropout)\n",
    "#         self.relu = nn.ReLU()\n",
    "        self.label = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input_sentence):\n",
    "        input = self.word_embeddings(input_sentence)\n",
    "        if input.shape[1] == self.batch_size:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, input.shape[1], self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(1, input.shape[1], self.hidden_size))        \n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input,(h_0, c_0))\n",
    "        print(output.size())\n",
    "        print(final_hidden_state.size())\n",
    "        print(final_cell_state.size())\n",
    "        print(\"----\")\n",
    "        final_output = self.label(final_hidden_state[-1])\n",
    "#         final_output = self.relu(final_output)\n",
    "#         final_output = self.fc_dropout(final_output)\n",
    "#         print(final_output)\n",
    "        return final_output\n",
    "#梯度裁剪来防止梯度爆炸\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        #nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def main():\n",
    "    model = LSTMClassifier()\n",
    "    model.train()\n",
    "    total_epoches_loss = 0\n",
    "    total_epoches_acc = 0\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    #动态监控lr,如多次(200)没有发生loss下降，则降低学习率\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 1e-3)\n",
    "    #传入优化器让学习器受其管理，当连续200次没有减少Loss时就减低lr(乘以0.9)\n",
    "    scheduler = StepLR(optimizer, step_size = 500, gamma = 0.9)\n",
    "    #pytorch中处理多分类用CrossEntropyLoss时，标签需从0开始\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    epoches = 5\n",
    "    Loss_list = []\n",
    "    Accuracy_list = []\n",
    "    for i in range(epoches):\n",
    "        each_batch_loss = 0\n",
    "        each_batch_acc = 0\n",
    "        for epoch, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(batch.content)\n",
    "            loss = loss_function(predicted, batch.category)\n",
    "            num_corrects = (torch.max(predicted, 1)[1].view(batch.category.size()).data\n",
    "                           == batch.category.data).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            clip_gradient(model, 1e-1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            each_batch_loss += loss.item()\n",
    "            each_batch_acc += acc.item()\n",
    "            Loss_list.append(each_batch_loss)\n",
    "            Accuracy_list.append(each_batch_acc)\n",
    "        total_epoches_loss += each_batch_loss\n",
    "        total_epoches_acc += each_batch_acc\n",
    "        print('第%d个epoch的loss值为%f'%(i+1, (each_batch_loss/len(train_iter))))\n",
    "        print('第%d个epoch的准确率为%f'%(i+1, (each_batch_acc/len(train_iter)/100.0)))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoches' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d28093da67ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#绘制loss值和acc值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAccuracy_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoches' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#绘制loss值和acc值\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = range(0, 10)\n",
    "x2 = range(0, 10)\n",
    "y1 = Accuracy_list\n",
    "y2 = Loss_list\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x1, y1, 'o-')\n",
    "plt.title('Train accuracy vs. epoches')\n",
    "plt.ylabel('Train accuracy')\n",
    "plt.plot(2, 1, 1)\n",
    "plt.plot(x2, y2, '.-')\n",
    "plt.title('Train loss vs. epoches')\n",
    "plt.ylabel('Train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------\"\"以下是原始代码-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集: 一共18种语言，加在一起有20074条数据\n",
    "import os\n",
    "file_path = r'C:\\Users\\lenovo\\Desktop\\Josie\\自学\\Pytorch_名字分类\\data\\data\\names'\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        category = file.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(os.path.join(root, file))\n",
    "        category_lines[category] = lines\n",
    "n_categories = len(all_categories)\n",
    "#将名字数据转为Tensor格式才能入模，pytorch在tensor上封装了遗赠variable\n",
    "#一个字母为一个张量\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "Embedding_length = 57\n",
    "embeds = nn.Embedding(n_letters, Embedding_length)\n",
    "def letterToTensor():\n",
    "    letter_Tensor = {}\n",
    "    for letter in all_letters:\n",
    "        letter_Tensor[letter] = embeds(Variable(\n",
    "                              torch.LongTensor([all_letters.find(letter)])))\n",
    "    return letter_Tensor\n",
    "def lineToTensor(line):\n",
    "    tensor = Variable(torch.randn(len(line), 1, Embedding_length))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li] = embeds(Variable(\n",
    "                              torch.LongTensor([all_letters.find(letter)])))\n",
    "    return tensor\n",
    "letterTensor = letterToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理标签值\n",
    "words = list(set(all_categories))\n",
    "word2ind = {word: i for i, word in enumerate(words)}\n",
    "words = list(set(all_categories))\n",
    "word2ind = {word: i for i, word in enumerate(words)}\n",
    "def Label_lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_categories)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][word2ind['Chinese']] = 1\n",
    "    tensor = Variable(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#随机采样训练样本对\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    line_tensor = lineToTensor(line)\n",
    "#     category_tensor = torch.squeeze(Label_lineToTensor(category))\n",
    "    category_tensor = Label_lineToTensor(category)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "# for i in range(5):\n",
    "#     category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "#     print(category, category_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=57,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "        )\n",
    "        self.linear = nn.Linear(128, 18)\n",
    " \n",
    "    def forward(self, x, h_n): \n",
    "        r_out, h_n = self.rnn(x, h_n)\n",
    "        outs = []\n",
    "        for step in range(r_out.size(1)):\n",
    "            outs.append(self.linear(r_out[:, step, :]))  \n",
    "        return torch.stack(outs, dim=1), h_n\n",
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "loss_func = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable(torch.randn(2, 3, 20))\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = Variable(torch.zeros(2, 1, 128))\n",
    "    rnn.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = rnn(line_tensor, hidden)\n",
    "    \n",
    "    #output去掉中间的batch_size维度，从3维变为2维\n",
    "#     output = torch.squeeze(output)\n",
    "    hidden = hidden.data\n",
    "    #分别为input_size, hidden_size, output_size, 真实label-size\n",
    "    print(line_tensor.size())\n",
    "    print(hidden.size())\n",
    "    print(output.size())\n",
    "    print(category_tensor.size())\n",
    "#     print(category_tensor.size())\n",
    "#     print(output.size())\n",
    "#     print(type(category_tensor))\n",
    "#     print(type(output))\n",
    "#     print(line_tensor)\n",
    "# train(category_tensor, line_tensor)\n",
    "#     计算损失值\n",
    "#     loss = loss_func(output, category_tensor)\n",
    "#     print('round' + str(i) + ' ' + str(loss))\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm(rnn.parameters(), 5)\n",
    "#     optimize.step()\n",
    "#     for p in rnn.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "#     return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 57])\n",
      "torch.Size([2, 1, 128])\n",
      "torch.Size([5, 1, 18])\n",
      "torch.Size([6, 1, 18])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-320-1407f68eae77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandomTrainingExample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mguess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguess_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategoryFromOutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "n_iters = 5000\n",
    "print_every = 200\n",
    "#定义类别\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1)\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "#这里开始迭代跑模型\n",
    "train_correct = []\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    #获取每批次的预判正确个数\n",
    "    if guess == category:\n",
    "        train_correct.append(1)\n",
    "    else:\n",
    "        train_correct.append(0)\n",
    "    correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "    if iter % print_every == 0:\n",
    "        print('%d %d%% %.4f %s / %s %s %s' % (iter, iter / n_iters * 100, loss, line, guess,\n",
    "                                          category, correct))\n",
    "print('准确率为%f'%(sum(train_correct)/(len(train_correct))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
